# python爬虫招聘网站
___
### 步骤

* 安装scrapy,selenium库，利用scrapy创建项目;

* 进入shell模式进入爬取网站，如果失败说明该网站具有反爬虫设置，则设置浏览器user-agent重新进入(response:200-表示成功);

* 基于网站内容，利用[xpath](https://github.com/vicjiafeng/python_application/blob/master/library/xpath.md)或css提取需要的信息亦或使用json，之后调用response的xpath()方法获取匹配结点，extract()提取结点内容

* 定义项目中items类(items.py)--对应属性为一个scrapy.field()

* 编写spider类(在项目目录下利用scrapy genspider 创建类文件用于爬取信息)，并根据需要爬取的信息更新文件内容

* 编写pipelines.py文件，负责将爬取的文件数据写入文件或数据库

* 修改settings.py文件，增加user-agent头

* spider已经开发完成，进入项目目录，执行`scrapy crawl xxx`启动spider

### 反爬虫举措

* 通过 User-Agent 请求头验证是否为浏览器

* 使用 JavaScript 动态加载资源

#### 网站源代码的页面的 <body.../> 元素可能是空的，通过`scrapy shell url`查看内容，之后服务器响应可能是json数据，由此分析网站

* 使用 IP 地址验证，程序会检查客户端的 IP 地址，如果发现同一个 IP 地址的客户端频繁地请求数据， 该网站就会判断该客户端是爬虫程序

#### Scrapy 不断地随机更换代理服务器的 IP 地址
  `打开 Scrapy 项目下的 middlewares.py 文件,添加get_random_proxy()，可随机返回代理ip；并在settings文件配置DOWNLOADER MIDDLEWARES `

* 通过跟踪 Cookie 来识别是否是同一个客户端

#### Scrapy 禁用 Cookie: `COOKIES_ENABLED = False`

* 很多 Web 站点目录下都会提供一个 robots.txt 文件，在该文件中制定了一系列爬虫规则

#### 让爬虫程序违反爬虫规则文件的限制，强行爬取站点信息，可以在 settings 文件中取消如下代码的注释来违反站点制定的爬虫规则:`ROBOTSTXT OBEY = False`

* 限制访问频率

#### 在settings文件中取消`AUTOTHROTTLE ENABLED = True`等注释

* 图形验证码，同一个客户端、同一个IP地址的访问次数，只要达到一定的访问次数，目标网站就会弹出一个图形验证码让你输入，只有成功输入了图形验证码才能继续访问

#### 第三方识别图片网站或自行用PIL库开发程序



